{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nutritional-hello",
   "metadata": {},
   "source": [
    "# Group project\n",
    "\n",
    "Importing the librarys that are to be used as well as gathering the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-video",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "import os\n",
    "import missingno as msno \n",
    "filelist = os.listdir('dataset') \n",
    "df_list = [pd.read_json('dataset/'+file, lines=True) for file in filelist]\n",
    "df = pd.concat(df_list).reset_index(drop=True, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-consent",
   "metadata": {},
   "source": [
    "## Fixing the \"none\" values\n",
    "\n",
    "*The dataset contains many of the same articles, however, not all of the articles are given the same category, documentId or publishtime. Therefore the fix_none function iterates over a given column, finds a not-none/null value and inserts it into the other articles.*\n",
    "\n",
    "`def fix_none(col_name)` is a function that takes in a column name and gathers all of the missing values: `.isna()` or all of the null values: `.isnull()`. After finding all of the null and missing values it drops its duplicates, using the 'url' field.\n",
    "\n",
    "---\n",
    "After gathering the requested columns and remove duplicates using the URL, the \"elements\" are extracted and consists of the rows that has a corresponding url value and a \"not missing\" value in the given column.\n",
    "\n",
    "---\n",
    "The last for-loop inserts the information gathered in the previous process into each row based on a corresponding url. So if there is a matching url, it will lookup in the \"elements\" dataframe and insert the correct value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_none(col_name):\n",
    "    d = df.loc[(df[col_name].isna()) | (df[col_name].isnull()), ['url', col_name]].drop_duplicates()\n",
    "    d = pd.DataFrame(d)\n",
    "    urls = list(d['url'])\n",
    "    \n",
    "    elements = pd.DataFrame(df.loc[df['url'].isin(urls) & (df[col_name].isna() == False), ['url', col_name]].drop_duplicates())\n",
    "    \n",
    "    for index, row in elements.iterrows():\n",
    "        df.loc[df['url'] == row['url'], col_name] = row[col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-walnut",
   "metadata": {},
   "source": [
    "## Fix all none\n",
    "\n",
    "The `def fix_all_none(col_names)` uses the `def fix_none(col_name)` function and iterates over a given list to remove duplicates and insert values if there exists any in the given dataset.\n",
    "\n",
    "---\n",
    "Here the columns `['category', 'documentId', 'publishtime']` will be used since these values should be the same for each of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_all_none(['category', 'documentId', 'publishtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['activeTime'].fillna(value=df['activeTime'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.loc[(df['category'].isna()) | (df['category'].isnull()), ['url', 'category']].drop_duplicates()\n",
    "pd.options.display.max_colwidth = 1000\n",
    "d = pd.DataFrame(d)\n",
    "urls = list(d['url'])\n",
    "# urls\n",
    "d\n",
    "# SELET url WHERE category = NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-constraint",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "categories = pd.DataFrame(df.loc[df['url'].isin(urls) & (df['category'].isna() == False), ['url', 'category', 'documentId']].drop_duplicates())\n",
    "# indexes = list(categories.index)\n",
    "# urls = list(urls)\n",
    "# categories = list(itertools.chain(*categories.values.tolist()))\n",
    "# for category, index, url in zip(categories, indexes, urls):\n",
    "#     df.loc[df['url'] == url, 'category'] = categories.loc[index]\n",
    "    \n",
    "#for index, row in categories.iterrows():\n",
    "#     df.loc[df['url'] == row['url'], 'category'] = row['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in categories.iterrows():\n",
    "    df.loc[df['url'] == row['url'], 'category'] = row['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://adressa.no/nyheter/trondheim/2017/03/17/dette-er-%c2%abkystad-saken%c2%bb-i-korte-trekk-og-sakens-konsekvenser-14463144.ece\n",
    "\n",
    "df.loc[df['url'].isin(['http://adressa.no/meninger/kronikker/2017/02/13/pasientblikket-nedenfra-14203543.ece'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-slovakia",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['url'].isin(['http://adressa.no/nyheter/sortrondelag/2017/03/20/nektet-straffskyld-for-%c3%a5-ha-kvalt-kameraten-til-d%c3%b8de-14472170.ece']), 'category'] = categories.loc[102]\n",
    "df[df['url'].isin(['http://adressa.no/nyheter/sortrondelag/2017/03/20/nektet-straffskyld-for-%c3%a5-ha-kvalt-kameraten-til-d%c3%b8de-14472170.ece'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['category'].isna(), 'url'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['url'] == 'http://adressa.no/vaeret/2017/03/20/vinterv%c3%a6r-igjen-midt-i-uken-betydelig-varmere-fra-helgen-14476360.ece', 'category'].drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
