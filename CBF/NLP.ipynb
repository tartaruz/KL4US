{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import NorwegianStemmer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vy111\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vy111\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = NorwegianStemmer() \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # self.nlp = spacy.load('nb_core_news_sm')\n",
    "        self.vectors = None\n",
    "\n",
    "\n",
    "    def preprosses_df(self):\n",
    "        self.info(\"Removing None and duplicates in data\")\n",
    "        self.data = self.data.drop_duplicates([\"title\"])\n",
    "        self.data = self.data.dropna(subset=('title',))\n",
    "        self.data = self.data.reset_index()\n",
    "\n",
    "        \n",
    "  \n",
    "    def get_titles(self):\n",
    "        self.info(\"Getting the titles\")\n",
    "        titles = self.data[\"title\"]\n",
    "        titles = [title for title in titles]\n",
    "        return titles\n",
    "\n",
    "    def remove_noice(self, titles):\n",
    "        self.info(\"Removing the noice in the titles\")\n",
    "        PUNCT = string.punctuation+\"«»\"\n",
    "        for index, title in enumerate(titles):\n",
    "            titles[index] = title.translate(str.maketrans('', '', PUNCT))\n",
    "        return titles\n",
    "\n",
    "    def normalize(self, titles):\n",
    "        self.info(\"Stemming the words\")\n",
    "        for index, title in enumerate(titles):\n",
    "            titles[index] = [stemmer.stem(word) for word in title]\n",
    "        return titles\n",
    "\n",
    "    def remove_stopwords(self, titles):\n",
    "        self.info(\"Removing the stopwords in the titles\")\n",
    "\n",
    "        for index, title in enumerate(titles):\n",
    "            titles[index] = [word for word in title if not word in stopwords.words(\"norwegian\")]\n",
    "        return titles\n",
    "\n",
    "    \n",
    "    def tokenize(self, titles):\n",
    "        self.info(\"Tokenizing and lowering the word in titles\")\n",
    "        for index in range(len(titles)):\n",
    "            titles[index] =  nltk.word_tokenize(titles[index].lower())\n",
    "        return titles\n",
    "\n",
    "    def do_magic(self):\n",
    "        self.info(\"Init prosses\")\n",
    "        self.preprosses_df()\n",
    "        titles = self.get_titles()\n",
    "\n",
    "        titles = self.remove_noice(titles)\n",
    "\n",
    "        titles = self.tokenize(titles)\n",
    "        \n",
    "        titles = self.remove_stopwords(titles)\n",
    "        \n",
    "        titles = self.normalize(titles)\n",
    "\n",
    "        self.data[\"new_title\"] = titles\n",
    "        self.TFIDF()\n",
    "\n",
    "    def TFIDF(self):\n",
    "        self.info(\"Vectorizing the corpus with TFIDF\")\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        corpus = [\" \".join(sentenc) for sentenc in self.data[\"new_title\"].tolist()]\n",
    "        self.vectors = vectorizer.fit_transform(corpus)\n",
    "        # denselist = vectors.todense().tolist()\n",
    "\n",
    "    def compare(self, title, k_number=10):\n",
    "        self.info(\"Comparing title with all other titles\")\n",
    "        self.info(f\"Search: [{title}]\")\n",
    "        q_index = self.data[self.data[\"title\"]==title].index\n",
    "        if len(q_index)<=0:\n",
    "            return None\n",
    "\n",
    "        similarity = cosine_similarity(self.vectors)[q_index].tolist()[0]\n",
    "        # for index,value in enumerate(similarity):\n",
    "        #     self.data.iloc[index][\"similarity\"] = value\n",
    "        # print(similarity)\n",
    "        # index, value = max(enumerate(similarity), key=operator.itemgetter(1))\n",
    "        # print(\"------------------\",index, value)\n",
    "        # print(self.data.iloc[index])\n",
    "        # self.data = self.data.sort_values(by=['index'])\n",
    "        self.data[\"similarity\"] = similarity\n",
    "        results = self.data.sort_values(by=['similarity'], ascending=False)\n",
    "        return results[:k_number]\n",
    "\n",
    "    def info(self, text):\n",
    "        line = \"=\"*(75-len(text))\n",
    "        print(f\"{line}=> {text} +  {len(self.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================> Init prosses +  680355\n",
      "========================================> Removing None and duplicates in data +  680355\n",
      "==========================================================> Getting the titles +  18958\n",
      "============================================> Removing the noice in the titles +  18958\n",
      "==================================> Tokenizing and lowering the word in titles +  18958\n",
      "========================================> Removing the stopwords in the titles +  18958\n",
      "==========================================================> Stemming the words +  18958\n",
      "===========================================> Vectorizing the corpus with TFIDF +  18958\n",
      "=======================================> Comparing title with all other titles +  18958\n",
      "=====================> Search: [Syltynt bak Håndball-Trondheims store stjerne] +  18958\n",
      "Syltynt bak Håndball-Trondheims store stjerne\n",
      "       similarity                                              title  \\\n",
      "13432    1.000000      Syltynt bak Håndball-Trondheims store stjerne   \n",
      "8458     0.338172                       Ingen stjerner til Trondheim   \n",
      "17317    0.272792                                Stjerne i en sesong   \n",
      "13074    0.269502          Norges stjerne klarer seg ikke uten disse   \n",
      "12482    0.211696                          En ny stjerne er i emning   \n",
      "7640     0.199221                         Her bakes det surdeigsbrød   \n",
      "2064     0.197936  Bruttern er bekymret for en av sine største st...   \n",
      "18024    0.196455   Disse stjernene kan hentes gratis etter sesongen   \n",
      "8888     0.195035              Stjernen er lei av å være middelmådig   \n",
      "8618     0.191365               Start håper å møte russiske stjerner   \n",
      "15277    0.184807      Trondheim får drahjelp på veien til stjernene   \n",
      "12258    0.183108     Så mye tjener de norske stjernene på triumfene   \n",
      "15223    0.182292       - Karoline er en kommende stjerne for Vipers   \n",
      "13547    0.175986     Tidligere Brann-talent debuterte med stjernene   \n",
      "5345     0.175560                      Alvoret bak glutenfri-trenden   \n",
      "\n",
      "                                                     url  \n",
      "13432  http://adressa.no/100sport/ballsport/syltynt-b...  \n",
      "8458   http://adressa.no/kultur/2017/02/22/ingen-stje...  \n",
      "17317  http://adressa.no/pluss/sport/2015/11/04/stjer...  \n",
      "13074  http://adressa.no/100sport/ballsport/disse-er-...  \n",
      "12482  http://adressa.no/100sport/fotball/en-ny-stjer...  \n",
      "7640         http://adressa.no/video/article14165146.ece  \n",
      "2064   http://adressa.no/100sport/fotball/bruttern-er...  \n",
      "18024  http://adressa.no/100sport/fotball/disse-stjer...  \n",
      "8888   http://adressa.no/100sport/vintersport/stjerne...  \n",
      "8618   http://adressa.no/100sport/fotball/start-haper...  \n",
      "15277  http://adressa.no/pluss/meninger/2016/10/04/tr...  \n",
      "12258  http://adressa.no/100sport/vintersport/sa-mye-...  \n",
      "15223  http://adressa.no/100sport/ballsport/--karolin...  \n",
      "13547  http://adressa.no/100sport/fotball/tidligere-b...  \n",
      "5345   http://adressa.no/pluss/nyheter/2016/03/27/alv...  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    path = \"data/pickeledDataset\"\n",
    "    df = pickle.load(open(path, \"rb\"))\n",
    "    nlp = NLP(df)\n",
    "\n",
    "    nlp.do_magic()\n",
    "\n",
    "    i = random.randint(0, len(nlp.data))\n",
    "    query = nlp.data[\"title\"][i]\n",
    "   \n",
    "    results = nlp.compare(query, k_number = 15)\n",
    "    cols = [\"similarity\",\"title\", \"url\"]\n",
    "    print(query)\n",
    "    print(results[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
